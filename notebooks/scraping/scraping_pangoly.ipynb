{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0607ce0f",
   "metadata": {},
   "source": [
    "# Web Scraping de Pangoly\n",
    "Este cuaderno tiene como objetivo recolectar datos a gran escala sobre placas base y sus componentes compatibles (Procesadores, Memoria RAM, Cajas y Disipadores) desde la web de Pangoly. Los datos extraidos se estructuran en un formato plano (JSONL) ideal para entrenar o afinar LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3d9ac",
   "metadata": {},
   "source": [
    "## 1. Importaciones y configuracion inicial\n",
    "En esta primera celda, importamos las librerias necesarias. Destaca el uso de `undetected_chromedriver` para evitar bloqueos por sistemas anti-bots (como Cloudflare) y `BeautifulSoup` para analizar el HTML rapidamente. Tambien definimos la funcion que inicializa nuestro navegador fantasma y una funcion auxiliar para limpiar y convertir los precios a formato numerico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Archivo de salida para los datos recolectados\n",
    "OUTPUT_FILE = \"pangoly.jsonl\"\n",
    "\n",
    "# Configura el undetected_chromedriver\n",
    "def setup_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# Limpia el texto del precio y lo convierte en float\n",
    "def parse_price(price_str):\n",
    "    if not price_str or price_str == \"N/A\":\n",
    "        return None\n",
    "    \n",
    "    # Eliminamos el simbolo de la divisa y posibles espacios\n",
    "    cleaned = price_str.replace('€', '').strip()\n",
    "    try:\n",
    "        return float(cleaned)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff27300",
   "metadata": {},
   "source": [
    "## 2. Motores de extraccion de datos\n",
    "Aqui definimos las dos funciones principales del \"scraper\":\n",
    "\n",
    "1. **`get_motherboards_from_page`**: Navega por el catalogo general de placas base. Extrae el nombre, el enlace, el \"slug\" (identificador de la URL) y el precio base de cada placa.\n",
    "\n",
    "2. **`get_compatible_components`**: Es el nucleo del scraper. Visita la pagina de cada categoria (CPU, RAM, etc) para una placa base especifica. Maneja la paginacion dinamica (AJAX) modificando el fragmento de la URL (`#page=X`) y utiliza \"esperas de tiempo\" (`WebDriverWait`) para asegurarse de que la tabla de compatibilidad se ha cargado en el navegador antes de extraer la marca, el modelo y el precio de los componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motherboards_from_page(driver, page_num):\n",
    "    url = f\"https://pangoly.com/en/browse/motherboard?page={page_num}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            lambda d: d.find_elements(By.CSS_SELECTOR, \"a.productItemLink\")\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    time.sleep(random.uniform(2.0, 3.5))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    items = soup.find_all('div', class_='productItem')\n",
    "    for item in items:\n",
    "        link_tag = item.find('a', class_='productItemLink')\n",
    "        if not link_tag:\n",
    "            continue\n",
    "            \n",
    "        link = link_tag.get('href')\n",
    "        header = link_tag.find('header')\n",
    "        name = header.text.strip() if header else link.split('/')[-1].replace('-', ' ').title()\n",
    "        \n",
    "        if link and not link.startswith('http'):\n",
    "            link = f\"https://pangoly.com{link}\"\n",
    "        slug = link.split('/')[-1]\n",
    "        \n",
    "        price_tag = item.find('span', title='Price')\n",
    "        raw_price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "        parsed_price = parse_price(raw_price)\n",
    "        \n",
    "        products.append({\n",
    "            \"name\": name, \n",
    "            \"url\": link, \n",
    "            \"slug\": slug, \n",
    "            \"price\": parsed_price\n",
    "        })\n",
    "            \n",
    "    return products\n",
    "\n",
    "def get_compatible_components(driver, slug, category, max_category_pages=None):\n",
    "    base_url = f\"https://pangoly.com/en/compatibility/{slug}/{category}\"\n",
    "    compatible_items = {} \n",
    "    current_page = 1\n",
    "    \n",
    "    while True:\n",
    "        url = base_url if current_page == 1 else f\"{base_url}#page={current_page}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        if current_page > 1:\n",
    "            time.sleep(1.0)\n",
    "            driver.refresh()\n",
    "            \n",
    "        try:\n",
    "            def is_page_ready(d):\n",
    "                if d.find_elements(By.CSS_SELECTOR, \"div.alert-info:not(.hidden)\"): \n",
    "                    return True\n",
    "                active_page = d.find_elements(By.CSS_SELECTOR, \"ul.pagination li.active span\")\n",
    "                if active_page and active_page[0].text.strip() == str(current_page):\n",
    "                    if d.find_elements(By.CSS_SELECTOR, \"table.table-striped tbody tr\"):\n",
    "                        return True\n",
    "                if current_page == 1:\n",
    "                    pagination = d.find_elements(By.CSS_SELECTOR, \"ul.pagination\")\n",
    "                    if not pagination and d.find_elements(By.CSS_SELECTOR, \"table.table-striped tbody tr\"):\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "            WebDriverWait(driver, 15).until(is_page_ready)\n",
    "            time.sleep(random.uniform(1.5, 3.0))\n",
    "            \n",
    "        except Exception:\n",
    "            print(f\"[!] Timeout o fin en {category} - Pagina {current_page}.\")\n",
    "            break\n",
    "            \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        alert = soup.find('div', class_='alert-info')\n",
    "        if alert and 'hidden' not in alert.get('class', []):\n",
    "            break\n",
    "            \n",
    "        table = soup.find('table', class_='table-striped')\n",
    "        if not table or not table.find('tbody'):\n",
    "            break\n",
    "            \n",
    "        rows = table.find('tbody').find_all('tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                link_tag = cols[1].find('a')\n",
    "                if link_tag and link_tag.find('strong'):\n",
    "                    model = link_tag.find('strong').text.strip()\n",
    "                    brand = \"\"\n",
    "                    for child in cols[1].contents:\n",
    "                        if child.name is None and child.strip():\n",
    "                            brand = child.strip()\n",
    "                            break \n",
    "                            \n",
    "                    full_name = f\"{brand} {model}\" if brand else model\n",
    "                    \n",
    "                    raw_price = \"N/A\"\n",
    "                    price_td = row.find('td', attrs={'data-label': 'Price'})\n",
    "                    if price_td:\n",
    "                        strong_price = price_td.find('strong')\n",
    "                        if strong_price:\n",
    "                            raw_price = strong_price.text.strip()\n",
    "                            \n",
    "                    parsed_price = parse_price(raw_price)\n",
    "                            \n",
    "                    if full_name not in compatible_items:\n",
    "                        compatible_items[full_name] = parsed_price\n",
    "        \n",
    "        if max_category_pages and current_page >= max_category_pages:\n",
    "            print(f\"(Limite de {max_category_pages} paginas alcanzado.)\", end=\"\")\n",
    "            break\n",
    "\n",
    "        next_li = soup.find('li', class_='PagedList-skipToNext')\n",
    "        if not next_li or 'disabled' in next_li.get('class', []):\n",
    "            break \n",
    "            \n",
    "        current_page += 1\n",
    "        \n",
    "    return [{\"name\": name, \"price\": price} for name, price in compatible_items.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b354af",
   "metadata": {},
   "source": [
    "## 3. Bucle principal y logica de guardado (Flat JSONL)\n",
    "Esta es la celda de ejecucion principal. El codigo coordina la recoleccion, iterando sobre las placas base y buscando sus componentes compatibles.\n",
    "\n",
    "**Caracteristicas clave de esta seccion:**\n",
    "* **Configuracion de limites:** Puedes definir `max_pages_to_scrape` y `max_category_pages` para limitar la recoleccion o dejarlos en `None` para raspar la web entera de forma autonoma.\n",
    "\n",
    "* **Guardado \"aplanado\" (Flat JSONL):** guarda una linea independiente por cada par placa-componente de manera incremental. Esto previene la perdida de datos y facilita la ingesta de los mismos por parte del modelo LLM.\n",
    "\n",
    "* **Cierre seguro:** Esta programado para atrapar excepciones de red. Si el usuario cierra la ventana de Chrome manualmente, el script lo detectara, guardara el progreso actual y finalizara el proceso limpiamente sin mostrar errores en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    driver = setup_driver()\n",
    "    categories_to_scrape = ['cpu', 'ram', 'case', 'cpu-cooler']\n",
    "    \n",
    "    # ---------------- IMPORTANTE!!! VARIABLES DE CONTROL DE LIMITES ------------------\n",
    "    start_page = 1             # Pagina inicial para comenzar el scraping\n",
    "    max_pages_to_scrape = None # 'None' = Placas bases infinitas\n",
    "    max_category_pages = None  # 'None' = Componentes infinitos\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    \n",
    "    page_num = start_page\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            if max_pages_to_scrape and page_num >= start_page + max_pages_to_scrape:\n",
    "                print(f\"\\nLimite de {max_pages_to_scrape} paginas del catalogo alcanzado.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"\\nExtrayendo catalogo de placas base: pagina {page_num}.\")\n",
    "            \n",
    "            try:\n",
    "                motherboards = get_motherboards_from_page(driver, page_num)\n",
    "            except WebDriverException:\n",
    "                print(\"\\n[!] El navegador se ha cerrado, el proceso de scraping se ha detenido.\")\n",
    "                break\n",
    "\n",
    "            if not motherboards:\n",
    "                print(f\"\\nNo se encontraron mas placas base en la página {page_num}. Catalogo completado!\")\n",
    "                break\n",
    "            \n",
    "            for mb in motherboards:\n",
    "                try: \n",
    "                    price_display = f\"€{mb['price']}\" if mb['price'] is not None else \"Sin stock/precio\"\n",
    "                    print(f\"\\n- {mb['name']} -\")\n",
    "                    \n",
    "                    for category in categories_to_scrape:\n",
    "                        print(f\" Buscando: {category.ljust(12)}\", end=\"\", flush=True)\n",
    "                        components = get_compatible_components(driver, mb['slug'], category, max_category_pages)\n",
    "                        print(f\"| {len(components)} modelos.\")\n",
    "                        \n",
    "                        # --- GUARDADO APLANADO EN EL ARCHIVO JSONL ---\n",
    "                        with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                            for comp in components:\n",
    "                                flat_record = {\n",
    "                                    \"motherboard\": mb['name'],\n",
    "                                    \"motherboard_price\": mb['price'],\n",
    "                                    \"currency\": \"EUR\",\n",
    "                                    \"component_type\": category,\n",
    "                                    \"component_name\": comp['name'],\n",
    "                                    \"component_price\": comp['price'],\n",
    "                                    \"compatible\": True\n",
    "                                }\n",
    "                                f.write(json.dumps(flat_record, ensure_ascii=False) + '\\n')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"disconnected\" in error_msg or \"closed\" in error_msg or \"not reachable\" in error_msg or \"refused\" in error_msg:\n",
    "                        print(\"\\n[!] El navegador se ha cerrado, el proceso de scraping se ha detenido.\")\n",
    "                        return \n",
    "                        \n",
    "                    print(f\"\\n[X] Error procesando placa {mb['name']}: {e}. Se ha saltado a la siguiente.\")\n",
    "                    continue \n",
    "            \n",
    "            page_num += 1\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[!] Proceso detenido por teclado (Ctrl+C), el proceso de scraping se ha detenido.\")\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit() \n",
    "        except:\n",
    "            pass\n",
    "        print(\"Scraping finalizado. Un saludo socio!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9a08a9",
   "metadata": {},
   "source": [
    "# Scraping PCPartPicker\n",
    "\n",
    "### üë®‚Äçüíª Autores del proyecto\n",
    "\n",
    "* [Alejandro Barrionuevo Rosado](https://github.com/Alejandro-BR)\n",
    "* [Alvaro L√≥pez Guerrero](https://github.com/Alvalogue72)\n",
    "* [Andrei Munteanu Popa](https://github.com/andu8705)\n",
    "\n",
    "M√°ster de FP en Inteligencia Artifical y Big Data - CPIFP Alan Turing - `Curso 2025/2026`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e674f",
   "metadata": {},
   "source": [
    "### 1. Importacion de librerias\n",
    "En esta celda importamos las herramientas necesarias. Usamos `undetected_chromedriver` como navegador principal para evadir la deteccion, `selenium` para la interaccion con elementos web, `pandas` para la estructura de datos, y `io.StringIO` para convertir el HTML en un formato legible para Pandas sin generar advertencias de depreciaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725033b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a96d2",
   "metadata": {},
   "source": [
    "### 2. Definicion de categorias y configuracion de limites\n",
    "Definimos un diccionario con las URLs de PCPartPicker que queremos analizar:\n",
    "\n",
    "1. Las claves del diccionario serviran para nombrar los DataFrames posteriormente. Se pueden a√±adir m√°s categorias siguiendo este mismo formato.\n",
    "\n",
    "2. Se ha utilizado la version espa√±ola (.es), pero se puede quitar si es que el usuario prefiere los precios en dolares americanos.\n",
    "\n",
    "Ademas, definimos el limite maximo de paginas que queremos recorrer y la ruta donde se guardara nuestro perfil de Chrome para evitar los bloqueos de Cloudflare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf059f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa float('inf') para que recolecte paginas de forma ilimitada\n",
    "MAX_PAGES = float('inf')\n",
    "\n",
    "CARPETA_PERFIL = os.path.join(os.path.expanduser(\"~\"), \"pcpartpicker_chrome_profile\")\n",
    "\n",
    "categories = {\n",
    "    'cpu': 'https://es.pcpartpicker.com/products/cpu/',\n",
    "    'cpu_cooler': 'https://es.pcpartpicker.com/products/cpu-cooler/',\n",
    "    'gpu': 'https://es.pcpartpicker.com/products/video-card/',\n",
    "    'ram': 'https://es.pcpartpicker.com/products/memory/',\n",
    "    'motherboard': 'https://es.pcpartpicker.com/products/motherboard/',\n",
    "    'storage': 'https://es.pcpartpicker.com/products/internal-hard-drive/',\n",
    "    'cases': 'https://es.pcpartpicker.com/products/case/',\n",
    "    'psu': 'https://es.pcpartpicker.com/products/power-supply/',\n",
    "    'os': 'https://es.pcpartpicker.com/products/os/',\n",
    "    'monitor': 'https://es.pcpartpicker.com/products/monitor/',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19981b5e",
   "metadata": {},
   "source": [
    "### 3. Configuracion del Driver\n",
    "Esta funcion inicializa el navegador:\n",
    "\n",
    "1. Configuramos el driver de Chrome para que se comporte como un usuario real.\n",
    "\n",
    "2. Se establece un tiempo de carga de pagina implicito para dar margen a la conexion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79452461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    if not os.path.exists(CARPETA_PERFIL):\n",
    "        os.makedirs(CARPETA_PERFIL)\n",
    "\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--no-first-run')\n",
    "    options.add_argument('--no-service-autorun')\n",
    "    options.add_argument('--password-store=basic')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    \n",
    "    # Inyecci√≥n de Perfil Persistente\n",
    "    options.add_argument(f\"--user-data-dir={CARPETA_PERFIL}\")\n",
    "    \n",
    "    driver = uc.Chrome(options=options, version_main=None)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "# Detecta si Cloudflare pide verificacion manual y pausa el script.\n",
    "def comprobar_cloudflare(driver):\n",
    "    \"\"\"Detecta si Cloudflare pide verificaci√≥n manual o si hay un Rate Limit.\"\"\"\n",
    "    try:\n",
    "        titulo = driver.title.lower()\n",
    "        # Detecci√≥n de Captchas de Cloudflare\n",
    "        if \"just a moment\" in titulo or \"cloudflare\" in titulo or \"attention required\" in titulo:\n",
    "            print(\"\\n[!] CLOUDFLARE DETECTADO\")\n",
    "            try:\n",
    "                WebDriverWait(driver, 120).until_not(EC.title_contains(\"Just a moment\"))\n",
    "                time.sleep(2)\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "                \n",
    "        # Deteccion de \"Soft-Bans\" por demasiadas peticiones (Error 429)\n",
    "        elif \"429\" in titulo or \"too many requests\" in titulo:\n",
    "            print(\"\\n[!] RATE LIMIT: Espera 60 segundos a que el script se reanude...\")\n",
    "            time.sleep(60)\n",
    "            driver.refresh() # Recargamos tras la pausa\n",
    "            time.sleep(5)\n",
    "            \n",
    "    except Exception:\n",
    "        raise # Si se cierra la ventana durante el chequeo, pasamos el error arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db3355",
   "metadata": {},
   "source": [
    "### 4. Scraping\n",
    "Esta es la funcion principal. Su logica es la siguiente:\n",
    "1. Navega a la URL.\n",
    "\n",
    "2. Espera a que la tabla de productos (`#category_content`) sea visible.\n",
    "\n",
    "3. Extrae el HTML de la tabla.\n",
    "\n",
    "4. Utiliza `StringIO` para envolver el texto HTML y pasarlo a `pd.read_html`, que convierte automaticamente la tabla HTML en un DataFrame.\n",
    "\n",
    "5. Usamos `BeautifulSoup` en el HTML extraido para buscar especificamente las etiquetas `<img>` dentro de las filas (`tr`).\n",
    "\n",
    "6. Extraemos el atributo `src` de la imagen e insertamos la lista de URLs como una columna nueva en el DataFrame.\n",
    "\n",
    "7. Realiza una limpieza basica: elimina columnas vacias (comunes en PCPartPicker por los botones de \"Add\") y filas nulas.\n",
    "\n",
    "**UPDATE:** Esta funcion ha sido optimizada. En lugar de encargarse de hacer clics en el boton \"Next\", ahora recibe una URL especifica de una pagina (`#page={num}`) y devuelve unicamente el DataFrame con los datos de esa pagina concreta. Esto nos permite controlar la paginacion de forma externa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_page(driver, url):\n",
    "    driver.get(url)\n",
    "    comprobar_cloudflare(driver)\n",
    "    \n",
    "    try:\n",
    "        # Esperamos a que la tabla sea visible\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.ID, \"category_content\"))\n",
    "        )\n",
    "        time.sleep(random.uniform(2.0, 4.0))\n",
    "        \n",
    "        table_element = driver.find_element(By.CSS_SELECTOR, \"table\")\n",
    "        table_html = table_element.get_attribute('outerHTML')\n",
    "        \n",
    "        # Transformamos el HTML crudo en un DataFrame con Pandas\n",
    "        html_buffer = StringIO(table_html)\n",
    "        df_page = pd.read_html(html_buffer)[0]\n",
    "        \n",
    "        # Extracci√≥n de URLs de im√°genes usando BeautifulSoup\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')  \n",
    "        rows = soup.find('tbody').find_all('tr')\n",
    "        \n",
    "        image_urls = []\n",
    "        for row in rows:\n",
    "            img_tag = row.find('img')\n",
    "            if img_tag and img_tag.get('src'):\n",
    "                src = img_tag.get('src')\n",
    "                if src.startswith('//'):\n",
    "                    src = 'https:' + src\n",
    "                image_urls.append(src)\n",
    "            else:\n",
    "                image_urls.append(None)\n",
    "        \n",
    "        # Alineamos las imagenes con el DataFrame\n",
    "        if len(df_page) == len(image_urls):\n",
    "            df_page['image_url'] = image_urls\n",
    "        else:\n",
    "            while len(image_urls) < len(df_page):\n",
    "                image_urls.append(None)\n",
    "            df_page['image_url'] = image_urls[:len(df_page)]\n",
    "\n",
    "        # Limpieza b√°sica\n",
    "        if not df_page.empty:\n",
    "            df_page = df_page.dropna(axis=1, how='all')\n",
    "            \n",
    "        return df_page\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cc67a",
   "metadata": {},
   "source": [
    "### 5. Main Loop\n",
    "En esta celda se orquesta todo el proceso:\n",
    "1. Iteramos pagina por p√°gina y dentro, iteramos por cada componente de la lista\n",
    "\n",
    "2. Si Pandas devuelve una tabla vac√≠a, significa que ese componente en particular ya no tiene mas paginas. Si esto pasa, se elimina esa categor√≠a de la lista para no perder tiempo en las siguientes vueltas.\n",
    "\n",
    "3. **Guardado:** Los datos se guardan linea por l√≠nea usando `orient='records', lines=True` de Pandas directamente en archivos `.jsonl` independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('pcpartpicker'):\n",
    "    os.makedirs('pcpartpicker')\n",
    "\n",
    "driver = setup_driver()\n",
    "categorias_activas = list(categories.keys())\n",
    "page = 1\n",
    "\n",
    "try:\n",
    "    # El bucle se mantiene mientras no alcancemos MAX_PAGES y queden categorias con datos\n",
    "    while page <= MAX_PAGES and categorias_activas:\n",
    "        print(f\"\\nScraping pagina {page} de categoria...\")\n",
    "        \n",
    "        # Iteramos sobre una copia de la lista para poder eliminar elementos de forma segura\n",
    "        for cat_name in categorias_activas.copy():\n",
    "            print(f\"\\nProcesando catalogo: {cat_name}\")\n",
    "            \n",
    "            # Construimos la URL din√°mica apoyandonos en el sistema de \"anclaje\" de PCPartPicker\n",
    "            url_dinamica = f\"{categories[cat_name]}#page={page}\"\n",
    "            \n",
    "            df_page = scrape_single_page(driver, url_dinamica)\n",
    "            \n",
    "            # Si Pandas no encuentra tabla, el cat√°logo ha llegado a su fin\n",
    "            if df_page is None or df_page.empty:\n",
    "                print(f\"\\nFin de datos para '{cat_name}'.\")\n",
    "                categorias_activas.remove(cat_name)\n",
    "            else:\n",
    "                # Guardado inmediato en formato JSONL\n",
    "                file_path = f\"pcpartpicker/{cat_name}_pcpartpicker.jsonl\"\n",
    "                \n",
    "                # Convertimos el DataFrame a texto JSONL plano\n",
    "                jsonl_str = df_page.to_json(orient='records', lines=True, force_ascii=False)\n",
    "                \n",
    "                with open(file_path, 'a', encoding='utf-8') as f:\n",
    "                    f.write(jsonl_str)\n",
    "                    if not jsonl_str.endswith('\\n'):\n",
    "                        f.write('\\n')\n",
    "                        \n",
    "                print(f\"Datos agregados!\")\n",
    "        \n",
    "        page += 1\n",
    "\n",
    "    print(\"\\nScript completado, un saludo socio!\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n[!] AVISO: El script ha sido detenido manualmente por el usuario.\")\n",
    "    print(\"[!] Los datos procesados se han guardado en el archivo JSONL.\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
